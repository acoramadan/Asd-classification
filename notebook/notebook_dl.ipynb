{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48cc60f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hop/home/muhammad_mufli_ramadhan/tfid/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "sys.path.append('..')\n",
    "from model.bert.bert_textdataset import BertTextDataset\n",
    "from model.bert.bert_classfier import BertFCClassifier\n",
    "from model.bert.bert_embedder import BertEmbedder\n",
    "from model.bert.bert_evaluator import BertFCEvaluator\n",
    "from model.bert.bert_trainer import BertFCTrainer\n",
    "from nlp_pipeline.preprocess_text import TextPreprocessor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nlp_pipeline.back_translator import BackTranslationAugmentor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7b3717e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/features/1.csv', encoding='latin1')\n",
    "df = df[['transcription', 'label']]\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "df = df[df['label'].isin(['ASD', 'NON ASD'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4a9699",
   "metadata": {},
   "source": [
    "AUGMENTED DATA. DO THIS IF NECESSARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2357491",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hop/home/muhammad_mufli_ramadhan/tfid/lib/python3.8/site-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2324 utterances for back translation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2324 [00:00<?, ?it/s]/hop/home/muhammad_mufli_ramadhan/tfid/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:4240: FutureWarning: \n",
      "`prepare_seq2seq_batch` is deprecated and will be removed in version 5 of HuggingFace Transformers. Use the regular\n",
      "`__call__` method to prepare your inputs and targets.\n",
      "\n",
      "Here is a short example:\n",
      "\n",
      "model_inputs = tokenizer(src_texts, text_target=tgt_texts, ...)\n",
      "\n",
      "If you either need to use different keyword arguments for the source and target texts, you should do two calls like\n",
      "this:\n",
      "\n",
      "model_inputs = tokenizer(src_texts, ...)\n",
      "labels = tokenizer(text_target=tgt_texts, ...)\n",
      "model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
      "\n",
      "See the documentation of your specific tokenizer for more details on the specific arguments to the tokenizer of choice.\n",
      "For a more complete example, see the implementation of `prepare_seq2seq_batch`.\n",
      "\n",
      "  warnings.warn(formatted_warning, FutureWarning)\n",
      "100%|██████████| 2324/2324 [14:13<00:00,  2.72it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of         label                   transcription  total_words  unique_words  \\\n",
      "0     NON ASD                Tunjuk kaka coba            3             3   \n",
      "1         ASD                           Inii!            2             2   \n",
      "2     NON ASD                   Siapa namamu?            4             4   \n",
      "3         ASD                         Iyaaaa?            2             2   \n",
      "4     NON ASD                  Kenalan duluu!            3             3   \n",
      "...       ...                             ...          ...           ...   \n",
      "5887  NON ASD                            Oke?            2             2   \n",
      "5888  NON ASD       Mari kita pergi ke bunga.            6             6   \n",
      "5889  NON ASD  Seperti Anda sudah cukup, ibu.            7             7   \n",
      "5890  NON ASD                Kiana lihat kaka            3             3   \n",
      "5891  NON ASD                Lihatlah saudara            3             3   \n",
      "\n",
      "      num_sentences  stopwords  num_adjectives  num_nouns  num_verbs  \\\n",
      "0                 1          1               0          0          2   \n",
      "1                 1          0               0          0          0   \n",
      "2                 1          1               0          1          0   \n",
      "3                 1          0               0          0          0   \n",
      "4                 1          0               0          1          1   \n",
      "...             ...        ...             ...        ...        ...   \n",
      "5887              1          0               0          0          0   \n",
      "5888              1          0               0          1          1   \n",
      "5889              1          1               0          1          0   \n",
      "5890              1          0               0          1          1   \n",
      "5891              1          0               0          1          1   \n",
      "\n",
      "      num_adverbs  type_token_ratio  avg_words_per_sentence  is_augmented  \n",
      "0               0               1.0                     3.0         False  \n",
      "1               0               1.0                     2.0         False  \n",
      "2               0               1.0                     4.0         False  \n",
      "3               0               1.0                     2.0         False  \n",
      "4               0               1.0                     3.0         False  \n",
      "...           ...               ...                     ...           ...  \n",
      "5887            0               1.0                     2.0          True  \n",
      "5888            0               1.0                     6.0          True  \n",
      "5889            1               1.0                     7.0          True  \n",
      "5890            0               1.0                     3.0          True  \n",
      "5891            0               1.0                     3.0          True  \n",
      "\n",
      "[5892 rows x 13 columns]>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/features/1.csv', encoding='latin1')\n",
    "columns = ['label', 'transcription', 'total_words', 'unique_words', 'num_sentences',\n",
    "                   'stopwords', 'num_adjectives', 'num_nouns', 'num_verbs', 'num_adverbs',\n",
    "                   'type_token_ratio', 'avg_words_per_sentence']\n",
    "df = df[columns]\n",
    "augmentor = BackTranslationAugmentor()\n",
    "df_augmented = augmentor.augment_dataframe(df)\n",
    "print(df_augmented.head)\n",
    "df_augmented.to_csv('../data/features/1_augmented.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd41ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_preprocessor = TextPreprocessor()\n",
    "df['clean_text'] = df['transcription'].apply(text_preprocessor.preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c3b9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = BertEmbedder()\n",
    "tokenizer = embedder.tokenizer\n",
    "dataset = BertTextDataset(df=df[['clean_text', 'label']], tokenizer=tokenizer)\n",
    "y = (df['label']== 'ASD').astype(int).values\n",
    "X = embedder.encode_series(df['clean_text'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe2b746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2854, 768) (2854,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "model = BertFCClassifier(input_dim=768, hidden_dim=128, dropout=0.3)\n",
    "\n",
    "trainer = BertFCTrainer(\n",
    "    model=model,\n",
    "    lr=2e-5,\n",
    "    batch_size=16,\n",
    "    epochs=20,\n",
    "    patience=3\n",
    ")\n",
    "print (X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67439628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sudah di update\n",
      "Epoch 1/20, Train Loss: 71.0762, Val Loss: 0.4246\n",
      "Epoch 2/20, Train Loss: 70.6640, Val Loss: 0.4212\n",
      "Epoch 3/20, Train Loss: 70.2911, Val Loss: 0.4217\n",
      "Epoch 4/20, Train Loss: 70.0362, Val Loss: 0.4201\n",
      "Epoch 5/20, Train Loss: 70.0063, Val Loss: 0.4193\n",
      "Epoch 6/20, Train Loss: 69.6845, Val Loss: 0.4197\n",
      "Epoch 7/20, Train Loss: 69.2611, Val Loss: 0.4184\n",
      "Epoch 8/20, Train Loss: 68.5228, Val Loss: 0.4181\n",
      "Epoch 9/20, Train Loss: 68.5416, Val Loss: 0.4176\n",
      "Epoch 10/20, Train Loss: 68.2078, Val Loss: 0.4171\n",
      "Epoch 11/20, Train Loss: 68.1157, Val Loss: 0.4166\n",
      "Epoch 12/20, Train Loss: 67.6746, Val Loss: 0.4169\n",
      "Epoch 13/20, Train Loss: 67.2832, Val Loss: 0.4162\n",
      "Epoch 14/20, Train Loss: 67.3722, Val Loss: 0.4159\n",
      "Epoch 15/20, Train Loss: 66.8521, Val Loss: 0.4156\n",
      "Epoch 16/20, Train Loss: 67.0199, Val Loss: 0.4157\n",
      "Epoch 17/20, Train Loss: 66.1453, Val Loss: 0.4149\n",
      "Epoch 18/20, Train Loss: 67.1085, Val Loss: 0.4152\n",
      "Epoch 19/20, Train Loss: 65.3855, Val Loss: 0.4146\n",
      "Epoch 20/20, Train Loss: 65.5944, Val Loss: 0.4141\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     NON ASD       0.86      0.85      0.85       465\n",
      "         ASD       0.73      0.73      0.73       249\n",
      "\n",
      "    accuracy                           0.81       714\n",
      "   macro avg       0.79      0.79      0.79       714\n",
      "weighted avg       0.81      0.81      0.81       714\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.train(\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_val=X_val,\n",
    "    y_val=y_val\n",
    ")\n",
    "trainer.evaluate(\n",
    "    X_test=X_val,\n",
    "    y_test=y_val\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecf759d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfid",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
