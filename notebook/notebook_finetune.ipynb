{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "568d8562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['d:\\\\aco\\\\research\\\\Asd-classification\\\\notebook', 'c:\\\\Users\\\\MufliDevs\\\\anaconda3\\\\python312.zip', 'c:\\\\Users\\\\MufliDevs\\\\anaconda3\\\\DLLs', 'c:\\\\Users\\\\MufliDevs\\\\anaconda3\\\\Lib', 'c:\\\\Users\\\\MufliDevs\\\\anaconda3', '', 'c:\\\\Users\\\\MufliDevs\\\\anaconda3\\\\Lib\\\\site-packages', 'c:\\\\Users\\\\MufliDevs\\\\anaconda3\\\\Lib\\\\site-packages\\\\win32', 'c:\\\\Users\\\\MufliDevs\\\\anaconda3\\\\Lib\\\\site-packages\\\\win32\\\\lib', 'c:\\\\Users\\\\MufliDevs\\\\anaconda3\\\\Lib\\\\site-packages\\\\Pythonwin', 'c:\\\\Users\\\\MufliDevs\\\\anaconda3\\\\Lib\\\\site-packages\\\\setuptools\\\\_vendor', '..', '..', '..', '..', '..', '..']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "print(sys.path)\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "from sklearn.utils import resample\n",
    "import pandas as pd\n",
    "from nlp_pipeline.preprocess_text import TextPreprocessor\n",
    "from nlp_pipeline.feature_extraction import FeatureExtractor\n",
    "from model.bert.bert_fusiondataset import BertFusionDataset\n",
    "from model.bert.bert_trainerfinetune import TrainerFusionBert\n",
    "from model.bert.bert_finetune import FineTuneBertWithLinguistic\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d366aea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"indobenchmark/indobert-base-p1\")\n",
    "MAX_LEN = 128\n",
    "\n",
    "def encode_texts(texts, max_len=MAX_LEN):\n",
    "    encodings = tokenizer(\n",
    "        list(texts),\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_len,\n",
    "        return_tensors='np'\n",
    "    )\n",
    "    return encodings['input_ids'], encodings['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ccf03511",
   "metadata": {},
   "outputs": [],
   "source": [
    "def oversample_token_ling(input_ids, att_mask, ling_feats, labels):\n",
    "    idx_min = np.where(labels == 1)[0]\n",
    "    idx_maj = np.where(labels == 0)[0]\n",
    "\n",
    "    ids_min = input_ids[idx_min]\n",
    "    mask_min = att_mask[idx_min]\n",
    "    ling_min = ling_feats[idx_min]\n",
    "    y_min = labels[idx_min]\n",
    "\n",
    "    ids_os, mask_os, ling_os, y_os = resample(\n",
    "        ids_min, mask_min, ling_min, y_min,\n",
    "        replace=True,\n",
    "        n_samples=len(idx_maj),\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    ids_final = np.concatenate([input_ids[idx_maj], ids_os], axis=0)\n",
    "    mask_final = np.concatenate([att_mask[idx_maj], mask_os], axis=0)\n",
    "    ling_final = np.vstack([ling_feats[idx_maj], ling_os])\n",
    "    y_final = np.concatenate([labels[idx_maj], y_os])\n",
    "\n",
    "    return ids_final, mask_final, ling_final, y_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c4cde1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcription</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tunjuk kaka coba</td>\n",
       "      <td>tunjuk kaka coba</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Inii!</td>\n",
       "      <td>ini</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Siapa namamu?</td>\n",
       "      <td>siapa nama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Iyaaaa?</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kenalan duluu!</td>\n",
       "      <td>kenal duluu</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      transcription        clean_text\n",
       "0  Tunjuk kaka coba  tunjuk kaka coba\n",
       "1             Inii!               ini\n",
       "2     Siapa namamu?        siapa nama\n",
       "3           Iyaaaa?                  \n",
       "4    Kenalan duluu!       kenal duluu"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df = pd.read_csv('../data/feature/combined_augmented_asd.csv', encoding='latin1')\n",
    "text_preprocessor = TextPreprocessor()\n",
    "df['clean_text'] = df['transcription'].apply(text_preprocessor.preprocess)\n",
    "df[['transcription', 'clean_text']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c96b7128",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce16dd64e9524ce08904231c1cf62913",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "197dab0d42554a609fa004eb5bdfe98c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a6ed0bdf3a247628f3c7ee11b53ae18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "decfb1d05e5b41eb8686d6b2b377a9ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96876c0041854b989ea74a3a4eac8ac1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aba030a5b454470abaa459ca14cf761d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f18309866804d73a40f889fb8addbb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f62aff878e341588afe35438b7ab1a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e21dd18f29945c1a2cc9bbff994e79d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b5aff845bfa4fdf92f9d17362c13f12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_ids, attention_mask = encode_texts(df['clean_text'])\n",
    "extractor = FeatureExtractor()\n",
    "X = extractor.encode_series_bert(df['clean_text'][:len(df['label'])])\n",
    "ling_cols = extractor.linguistic_cols\n",
    "ling_feats = df[ling_cols].values\n",
    "labels = (df['label'].str.upper().str.strip() == 'ASD').astype(int).values\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "ids_train, ids_val, mask_train, mask_val, ling_train, ling_val, y_train, y_val = train_test_split(\n",
    "    input_ids, attention_mask, ling_feats, labels,\n",
    "    stratify=labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "ids_train_os, mask_train_os, ling_train_os, y_train_os = oversample_token_ling(\n",
    "    ids_train, mask_train, ling_train, y_train\n",
    ")\n",
    "\n",
    "train_dataset = BertFusionDataset(ids_train_os, mask_train_os, ling_train_os, y_train_os)\n",
    "val_dataset = BertFusionDataset(ids_val, mask_val, ling_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5df69088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "  Train Loss: 0.3830\n",
      "  Val Loss  : 0.3197\n",
      "  Val Acc   : 0.8616 | Precision: 0.8237 | Recall: 0.8767 | F1: 0.8494\n",
      "Epoch 2:\n",
      "  Train Loss: 0.2605\n",
      "  Val Loss  : 0.3052\n",
      "  Val Acc   : 0.8759 | Precision: 0.8346 | Recall: 0.8995 | F1: 0.8658\n",
      "Epoch 3:\n",
      "  Train Loss: 0.2202\n",
      "  Val Loss  : 0.2904\n",
      "  Val Acc   : 0.8723 | Precision: 0.8284 | Recall: 0.8995 | F1: 0.8625\n",
      "Epoch 4:\n",
      "  Train Loss: 0.1940\n",
      "  Val Loss  : 0.3149\n",
      "  Val Acc   : 0.8872 | Precision: 0.8566 | Recall: 0.8968 | F1: 0.8762\n",
      "Epoch 5:\n",
      "  Train Loss: 0.1978\n",
      "  Val Loss  : 0.3109\n",
      "  Val Acc   : 0.8878 | Precision: 0.8577 | Recall: 0.8968 | F1: 0.8768\n",
      "Early stopping triggered.\n"
     ]
    }
   ],
   "source": [
    "model = FineTuneBertWithLinguistic(\n",
    "    model_name=\"indobenchmark/indobert-base-p1\",\n",
    "    ling_dim=10\n",
    ")\n",
    "\n",
    "trainer = TrainerFusionBert(model, device='cpu', epochs=10, patience=2)\n",
    "model = trainer.train(train_dataset, val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d8dc7059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Metrics: {'accuracy': 0.8878281622911695, 'precision': 0.8576923076923076, 'recall': 0.8967828418230563, 'f1': 0.8768020969855832}\n"
     ]
    }
   ],
   "source": [
    "test_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "_, test_metrics = trainer.evaluate(test_loader)\n",
    "print(\"Validation Metrics:\", test_metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
